{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68e44cdf-8743-4dc4-8ad0-77ec15525b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "from utils.tinyyolov2 import TinyYoloV2Fused\n",
    "from utils.camera import CameraDisplay, CameraDetectionDisplay\n",
    "from utils.yolo import nms, filter_boxes\n",
    "from utils.viz import display_result\n",
    "from utils.dataloader import num_to_class\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c4411ba-db29-4077-b561-a62ba77c6c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://github.com/NVIDIA/Torch-TensorRT/releases\n",
      "Collecting torch-tensorrt\n",
      "  Using cached torch-tensorrt-0.0.0.post1.tar.gz (9.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /usr/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-4l9xwjel/torch-tensorrt_f2f82d6d22fe453196952714f74e2fbc/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-4l9xwjel/torch-tensorrt_f2f82d6d22fe453196952714f74e2fbc/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-qn7gh6tk\n",
      "       cwd: /tmp/pip-install-4l9xwjel/torch-tensorrt_f2f82d6d22fe453196952714f74e2fbc/\n",
      "  Complete output (12 lines):\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/tmp/pip-install-4l9xwjel/torch-tensorrt_f2f82d6d22fe453196952714f74e2fbc/setup.py\", line 125, in <module>\n",
      "      raise RuntimeError(open(\"ERROR.txt\", \"r\").read())\n",
      "  RuntimeError:\n",
      "  ###########################################################################################\n",
      "  The package you are trying to install is only a placeholder project on PyPI.org repository.\n",
      "  To install Torch-TensorRT please run the following command:\n",
      "  \n",
      "  $ pip install torch-tensorrt -f https://github.com/NVIDIA/Torch-TensorRT/releases\n",
      "  ###########################################################################################\n",
      "  \n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/f3/5e/81c357454c59acedb1b83bfa1e2032f95bde827b5631bb695af6911f5a14/torch-tensorrt-0.0.0.post1.tar.gz#sha256=ebe119f7783ebd4ef4e44815c0a3014941172a2deb4f325ffc410369891138d7 (from https://pypi.org/simple/torch-tensorrt/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch-tensorrt (from versions: 0.0.0, 0.0.0.post1)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torch-tensorrt\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install torch-tensorrt -f https://github.com/NVIDIA/Torch-TensorRT/releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73435e66-ddec-447d-94d3-7aae9711e86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d0e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99c3fc9b-5340-4dde-9092-789bbd69a001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "run_with_pytorch = False\n",
    "\n",
    "load_path = \"runs/taylor-new/voc_pruned_7_finetuned.pt\"\n",
    "\n",
    "if run_with_pytorch:\n",
    "    \n",
    "    net = TinyYoloV2Fused(num_classes=1)\n",
    "    net.load_state_dict(torch.load(load_path), strict=False)\n",
    "    net.fuse_after_loading_sd()\n",
    "    net.eval()\n",
    "    \n",
    "    torch.onnx.export(net, torch.zeros(1, 3, 320, 320), f\"{load_path}.onnx\", opset_version=11, input_names = ['input'], output_names = ['output'])\n",
    "    \n",
    "    net.to(device)\n",
    "    print(net)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    sess_options = ort.SessionOptions()\n",
    "    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    \n",
    "    session = ort.InferenceSession(f\"{load_path}.onnx\", sess_options, providers=['CUDAExecutionProvider'])\n",
    "    binding = session.io_binding()\n",
    "\n",
    "    device_name = 'cuda'\n",
    "    output_orig = torch.empty((1, 5, 10, 10, 5 + 1), dtype=torch.float32, device=torch.device(\"cpu\"))\n",
    "    binding.bind_output(\n",
    "        name = 'output',\n",
    "        device_type = \"cpu\",\n",
    "        device_id = 0,\n",
    "        element_type = np.float32,\n",
    "        shape = output_orig.shape,\n",
    "        buffer_ptr = output_orig.data_ptr()\n",
    "    )\n",
    "\n",
    "    print(session.get_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b453b17-826b-4ed9-8857-4cad3fdd3a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_inference(net, image):\n",
    "    input = to_tensor(image).unsqueeze(0).to(device)\n",
    "    output = None\n",
    "    \n",
    "    if run_with_pytorch:\n",
    "        \n",
    "        # torch.inference_mode() doesnt work\n",
    "        with torch.no_grad(): \n",
    "            output = net(input)\n",
    "        output = output.cpu()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        binding.bind_input(\n",
    "            name = 'input',\n",
    "            device_type = device_name,\n",
    "            device_id = 0,\n",
    "            element_type = np.float32,\n",
    "            shape = input.shape,\n",
    "            buffer_ptr = input.data_ptr()\n",
    "        )\n",
    "        session.run_with_iobinding(binding)\n",
    "        output = output_orig\n",
    "\n",
    "    output = filter_boxes(output, 0.3)\n",
    "    output = nms(output, 0.25)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7be1fcfd-7b00-4d00-be96-b77d19dc815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bboxes(image, output):\n",
    "    img_shape = 320\n",
    "    \n",
    "    bboxes = torch.stack(output, dim=0)\n",
    "    \n",
    "    for i in range(bboxes.shape[1]):\n",
    "\n",
    "        # only show person\n",
    "        # if int(bboxes[0,i,5]) != 14:\n",
    "        #    continue\n",
    "        \n",
    "        if bboxes[0,i,-1] >= 0:\n",
    "            \n",
    "            cx = int(bboxes[0,i,0]*img_shape - bboxes[0,i,2]*img_shape/2)\n",
    "            cy = int(bboxes[0,i,1]*img_shape - bboxes[0,i,3]*img_shape/2)\n",
    "\n",
    "            w = int(bboxes[0,i,2]*img_shape)\n",
    "            h = int(bboxes[0,i,3]*img_shape)\n",
    "            \n",
    "            cv2.rectangle(image, (cx, cy), (cx + w, cy + h), color=(255,0,0), thickness=2)\n",
    "\n",
    "            # annotation = num_to_class(int(bboxes[0,i,5])) + \" \"+  f\"{float(bboxes[0,i,4]):.2f}\"\n",
    "            annotation = f\"{float(bboxes[0,i,4]):.2f}\"\n",
    "            \n",
    "            cv2.putText(image, annotation, (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1173f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "\n",
    "def apply_fps(image):\n",
    "    global now\n",
    "\n",
    "    fps = f\"{int(1/(time.time() - now))}\"\n",
    "    now = time.time()\n",
    "\n",
    "    cv2.putText(image, f\"{fps}fps\", (2, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f273cb05-1fa3-45cf-b637-66a6e22dbea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(net, image):\n",
    "    image = image[0:320,0:320, :]\n",
    "    output = do_inference(net if run_with_pytorch else None, image)\n",
    "\n",
    "    image = apply_bboxes(image, output)\n",
    "    image = apply_fps(image)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3862254a-5f10-4d50-a109-fae94009e508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing camera...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c24240386104e48835b5d6b6982c0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cam = CameraDetectionDisplay(net if run_with_pytorch else None, callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14273897-5026-459e-ac79-a8e372b44529",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9baf3b9-a78c-4709-a1df-4fabd57366a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera released\n"
     ]
    }
   ],
   "source": [
    "# The camera should always be stopped and released for a new camera is instantiated (calling CameraDisplay(callback) again)\n",
    "cam.stop()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f6003-9e61-4d2a-ac8f-32a545c4db65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
